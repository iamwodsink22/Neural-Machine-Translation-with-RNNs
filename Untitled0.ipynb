{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tiw8Ha6blwaM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "ENGLISH_SEQ_LEN=64\n",
        "SPA_SEQ_LEN=64\n",
        "VOCAB_SIZE=20000\n",
        "\n",
        "dataset_init=tf.data.TextLineDataset(\"spa.txt\")\n",
        "eng_vectorization_layer=keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',max_tokens=VOCAB_SIZE,output_mode='int',output_sequence_length=ENGLISH_SEQ_LEN)\n",
        "spa_vectorization_layer=keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',max_tokens=VOCAB_SIZE,output_mode='int',output_sequence_length=SPA_SEQ_LEN)\n",
        "\n",
        "\n",
        "\n",
        "def selector(input):\n",
        "    split_text=tf.strings.split(input,'\\t')\n",
        "    return {'input_1':split_text[0:1],'input_2':'[start] '+split_text[1:2]},split_text[1:2]+' [end]'\n",
        "def initselector(input):\n",
        "    split_text=tf.strings.split(input,'\\t')\n",
        "    return split_text[0:1],'[start] '+split_text[1:2]+' [end]'\n",
        "def vectorizer(inputs,outputs):\n",
        "     return {'input_1':eng_vectorization_layer(inputs['input_1']),'input_2':spa_vectorization_layer(inputs['input_2'])},spa_vectorization_layer(outputs)\n",
        "\n",
        "split_dataset=dataset_init.map(selector)\n",
        "init_dataset=dataset_init.map(initselector)\n",
        "eng_dataset=init_dataset.map(lambda x,y:x)\n",
        "eng_vectorization_layer.adapt(eng_dataset)\n",
        "spa_dataset=init_dataset.map(lambda x,y:y)\n",
        "spa_vectorization_layer.adapt(spa_dataset)\n",
        "\n",
        "dataset=split_dataset.map(vectorizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "NUM_LAYERS=4\n",
        "BATCH_SIZE=64\n",
        "\n",
        "EMBEDDING_DIM=256\n",
        "DENSE_DIM=2048\n",
        "SEQUENCE_LEN=64\n",
        "\n",
        "\n",
        "NUM_BATCHES=int(200000/BATCH_SIZE)\n",
        "dataset=dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "train_dataset=dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset=dataset.skip(int(0.9*NUM_BATCHES ))"
      ],
      "metadata": {
        "id": "u5YF5cEtl6dk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUuSsJbtQAY",
        "outputId": "4575ceef-c2b7-4b57-835c-fc7dbdd94b4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_UNITS=256\n",
        "EMBEDDING_DIM=256\n",
        "input=keras.layers.Input(shape=(ENGLISH_SEQ_LEN),dtype='int64',name='input_1')\n",
        "x=keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM)(input)\n",
        "y=keras.layers.Bidirectional(keras.layers.GRU(NUM_UNITS),)(x)\n",
        "\n",
        "\n",
        "shifted_target=keras.layers.Input(shape=(SPA_SEQ_LEN),dtype='int64',name='input_2')\n",
        "a=keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM)(shifted_target)\n",
        "b=keras.layers.GRU(NUM_UNITS*2,return_sequences=True)(a,initial_state=y)\n",
        "x=keras.layers.Dropout(0.4)(b)\n",
        "output=keras.layers.Dense(VOCAB_SIZE,activation='softmax')(x)\n",
        "\n",
        "myRNN=keras.models.Model([input,shifted_target],output)\n",
        "\n",
        "print(myRNN.summary())"
      ],
      "metadata": {
        "id": "2iICB3atl-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(sentence):\n",
        "    tokenized_eng=eng_vectorization_layer([sentence])\n",
        "    shifted_target='starttoken'\n",
        "    for i in range(SPA_SEQ_LEN):\n",
        "        tokenized_spa=spa_vectorization_layer([shifted_target])\n",
        "        spa_sent=myRNN.predict([tokenized_eng,tokenized_spa])\n",
        "        out=tf.argmax(spa_sent,axis=-1)[0][i].numpy()\n",
        "        word=index_to_word[out]\n",
        "\n",
        "        if word=='end':\n",
        "            break\n",
        "\n",
        "        shifted_target+=' '+word\n",
        "    return shifted_target[11:]\n"
      ],
      "metadata": {
        "id": "Xe_WfGRVl_8Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word={x:y for x,y in zip(range(len(spa_vectorization_layer.get_vocabulary())),spa_vectorization_layer.get_vocabulary())}\n"
      ],
      "metadata": {
        "id": "aEbDPchNmG1V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkdir='dir to save checkpoints'\n",
        "\n",
        "#You can load the half-trained model if you would like to.\n",
        "# myRNN=keras.models.load_model(\"/content/drive/MyDrive/076BCT010-ARAKSHA/RNNFinal.keras\")\n",
        "callback_checkpoint=keras.callbacks.ModelCheckpoint(checkdir,mod='max',monitor='val_accuracy',save_weights_only=True)\n",
        "\n",
        "myRNN.compile(optimizer=keras.optimizers.Adam(5e-4),loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "myRNN.fit(train_dataset,validation_data=val_dataset,epochs=5,callbacks=[callback_checkpoint])\n"
      ],
      "metadata": {
        "id": "uuJt47y-mJB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator(\"You are an idiot\")"
      ],
      "metadata": {
        "id": "Cd9ZO3OlpIsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "147c6d7d-6b3e-4279-97b9-aed6047748c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ella es [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "tf.version.VERSION"
      ],
      "metadata": {
        "id": "j8lavpcB8XTf",
        "outputId": "27387732-3165-41af-b017-f804d0b37944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOUuJWfGSw0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}